<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>现代信息检索 Chapter 3：信息检索建模 | SHIVAKASU</title><meta name="description" content="现代信息检索 Chapter 3：信息检索建模"><meta name="keywords" content="计算机理论"><meta name="author" content="w.k.x.,wkx1996@foxmail.com"><meta name="copyright" content="w.k.x."><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="https://file.shivakasu.cn/eb8581b76ec032ab0db8/favicon.ico"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin><link rel="preconnect" href="//busuanzi.ibruce.info"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="现代信息检索 Chapter 3：信息检索建模"><meta name="twitter:description" content="现代信息检索 Chapter 3：信息检索建模"><meta name="twitter:image" content="https://file.shivakasu.cn/431c4e80e82294b29130/mir.png"><meta property="og:type" content="article"><meta property="og:title" content="现代信息检索 Chapter 3：信息检索建模"><meta property="og:url" content="http://shivakasu.github.io/2019/01/24/mir2/"><meta property="og:site_name" content="SHIVAKASU"><meta property="og:description" content="现代信息检索 Chapter 3：信息检索建模"><meta property="og:image" content="https://file.shivakasu.cn/431c4e80e82294b29130/mir.png"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>const autoChangeMode = 'false'
var t = Cookies.get("theme");
if (autoChangeMode == '1'){
const isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
const isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
const isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

if (t === undefined){
  if (isLightMode) activateLightMode()
  else if (isDarkMode) activateDarkMode()
  else if (isNotSpecified || hasNoSupport){
    console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
    now = new Date();
    hour = now.getHours();
    isNight = hour < 6 || hour >= 18
    isNight ? activateDarkMode() : activateLightMode()
}
} else if (t == 'light') activateLightMode()
else activateDarkMode()


} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="canonical" href="http://shivakasu.github.io/2019/01/24/mir2/"><link rel="prev" title="现代信息检索 Chapter 4：检索评价" href="http://shivakasu.github.io/2019/02/01/mir3/"><link rel="next" title="现代信息检索 Chapter 2：用户搜索界面" href="http://shivakasu.github.io/2019/01/19/mir1/"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.js" defer></script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"QBXC0PXLIT","apiKey":"517431eabeeedb8d3792391b21e8cf20","indexName":"blog","hits":{"per_page":6},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容:${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    title: 'Snackbar.bookmark.title',
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  baiduPush: false,
  isHome: false,
  isPost: true
  
}</script><meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="SHIVAKASU" type="application/atom+xml">
</head><body><canvas class="fireworks"></canvas><header> <div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">SHIVAKASU</a></span><span class="toggle-menu pull_right close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-list" aria-hidden="true"></i><span> List</span><i class="fa fa-chevron-down menus-expand" aria-hidden="true"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fa fa-music"></i><span> Music</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fa fa-film"></i><span> Movie</span></a></li><li><a class="site-page" href="/books/"><i class="fa-fw fa fa-book"></i><span> Book</span></a></li></ul></div></div></span><span class="pull_right" id="search_button"><a class="site-page social-icon search"><i class="fa fa-search fa-fw"></i><span> Search</span></a></span></div></header><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="https://file.shivakasu.cn/cb7049104af4685e7289/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">29</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">11</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">6</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-list" aria-hidden="true"></i><span> List</span><i class="fa fa-chevron-down menus-expand" aria-hidden="true"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fa fa-music"></i><span> Music</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fa fa-film"></i><span> Movie</span></a></li><li><a class="site-page" href="/books/"><i class="fa-fw fa fa-book"></i><span> Book</span></a></li></ul></div></div></div><div id="mobile-sidebar-toc"><div class="toc_mobile_headline">目录</div><div class="sidebar-toc__content"><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#1-信息检索模型"><span class="toc_mobile_items-text">1 信息检索模型</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#2-经典信息检索"><span class="toc_mobile_items-text">2 经典信息检索</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#2-1-基本概念"><span class="toc_mobile_items-text">2.1 基本概念</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#2-2-布尔模型"><span class="toc_mobile_items-text">2.2 布尔模型</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#2-3-项权重"><span class="toc_mobile_items-text">2.3 项权重</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#2-4-TD-IDF"><span class="toc_mobile_items-text">2.4 TD-IDF</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#2-5-文档长度归一化"><span class="toc_mobile_items-text">2.5 文档长度归一化</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#2-6-向量模型"><span class="toc_mobile_items-text">2.6 向量模型</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#2-7-概率模型"><span class="toc_mobile_items-text">2.7 概率模型</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#3-其他集合论模型"><span class="toc_mobile_items-text">3 其他集合论模型</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#3-1-基于集合的模型"><span class="toc_mobile_items-text">3.1 基于集合的模型</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#3-2-扩展布尔模型"><span class="toc_mobile_items-text">3.2 扩展布尔模型</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#3-3-模糊集模型"><span class="toc_mobile_items-text">3.3 模糊集模型</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#4-其他代数模型"><span class="toc_mobile_items-text">4 其他代数模型</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#4-1-广义向量空间模型"><span class="toc_mobile_items-text">4.1 广义向量空间模型</span></a></li></ol></li></ol></div></div></div><div id="body-wrap"><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true">     </i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-信息检索模型"><span class="toc-text">1 信息检索模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-经典信息检索"><span class="toc-text">2 经典信息检索</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-基本概念"><span class="toc-text">2.1 基本概念</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-布尔模型"><span class="toc-text">2.2 布尔模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-项权重"><span class="toc-text">2.3 项权重</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-TD-IDF"><span class="toc-text">2.4 TD-IDF</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-5-文档长度归一化"><span class="toc-text">2.5 文档长度归一化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-6-向量模型"><span class="toc-text">2.6 向量模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-7-概率模型"><span class="toc-text">2.7 概率模型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-其他集合论模型"><span class="toc-text">3 其他集合论模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-基于集合的模型"><span class="toc-text">3.1 基于集合的模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-扩展布尔模型"><span class="toc-text">3.2 扩展布尔模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-模糊集模型"><span class="toc-text">3.3 模糊集模型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-其他代数模型"><span class="toc-text">4 其他代数模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-广义向量空间模型"><span class="toc-text">4.1 广义向量空间模型</span></a></li></ol></li></ol></div></div></div><main id="content-outer"><div id="top-container" style="background-image: url(https://file.shivakasu.cn/431c4e80e82294b29130/mir.png)"><div id="post-info"><div id="post-title"><div class="posttitle">现代信息检索 Chapter 3：信息检索建模</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 发表于 2019-01-24<span class="post-meta__separator">|</span><i class="fa fa-history fa-fw" aria-hidden="true"></i> 更新于 2020-01-15</time><span class="post-meta__separator">|</span><span><i class="fa fa-inbox post-meta__icon fa-fw" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">读书笔记</a></span><div class="post-meta-wordcount"><i class="fa fa-file-word-o post-meta__icon fa-fw" aria-hidden="true"></i><span>字数总计:</span><span class="word-count">4.1k</span><span class="post-meta__separator">|</span><i class="fa fa-clock-o post-meta__icon fa-fw" aria-hidden="true"></i><span>阅读时长: 14 分钟</span><div class="post-meta-pv-cv"><span class="post-meta__separator">|</span><span><i class="fa fa-eye post-meta__icon fa-fw" aria-hidden="true"> </i>阅读量:</span><span id="busuanzi_value_page_pv"></span><span class="post-meta__separator">|</span><i class="fa fa-comments-o post-meta__icon fa-fw" aria-hidden="true"></i><span>评论数:</span><a href="/2019/01/24/mir2/#post-comment" itemprop="discussionUrl"><span class="valine-comment-count comment-count" data-xid="/2019/01/24/mir2/" itemprop="commentCount"></span></a></div></div></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><h2 id="1-信息检索模型"><a href="#1-信息检索模型" class="headerlink" title="1 信息检索模型"></a>1 信息检索模型</h2><p>&emsp;&emsp;信息检索模型是一个四元组 <script type="math/tex">[D,Q,F,R(q_i,d_i)]</script> 。其中D(Document)是文档表示的集合，Q(Query)是查询表示的集合，F(Framework)是对文档、查询及其关系建模的框架，例如布尔框架、线性框架，R(Ranking)是排序函数，对于查询表达式 <script type="math/tex">q_i\in Q</script> 和文档表达式 <script type="math/tex">d_i\in D</script> ，函数返回文档 <script type="math/tex">d_i</script> 关于查询<script type="math/tex">q_i</script>的文档次序。</p>
<p>&emsp;&emsp;信息检索主要基于文本，文本模型也细分为是否考虑文本结构，所谓考虑文本结构就是区别对待标题、段落等文档不同结构处的内容。在web中，由于文档数量巨大，还需要考虑网页之间的链接，如今的web排序函数结合了经典的信息检索模型和基于链接模型的特征来提高检索性能。信息检索还可基于图像、音频等多媒体数据，检索策略更复杂。信息检索模型的分类体系见下图：<br><a href="https://file.shivakasu.cn/648d8d4d85d3f6ed9f89/mir2-0.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="mir2-0" class="fancybox"><img alt="mir2-0" data-src="https://file.shivakasu.cn/648d8d4d85d3f6ed9f89/mir2-0.png" class="lazyload" title="mir2-0"></a></p>
<h2 id="2-经典信息检索"><a href="#2-经典信息检索" class="headerlink" title="2 经典信息检索"></a>2 经典信息检索</h2><h3 id="2-1-基本概念"><a href="#2-1-基本概念" class="headerlink" title="2.1 基本概念"></a>2.1 基本概念</h3><ul>
<li>索引项(index term)：文档里的一个词或一组连续的词，主要是名词，因为名词相比于形容词、副词等往往能包含更多信息，具体的选择策略因人而异。</li>
<li>词汇表(vocabulary)： <script type="math/tex">V=(k_1,k_2,\cdots ,k_t)</script> ，其中 <script type="math/tex">t</script> 是文档集中索引项的数量， <script type="math/tex">k_i</script> 是某个索引项。</li>
<li>文档表示：就是简单的词袋方法，表示成和词汇表一样长的向量，其中每个元素是0或1，0表示对应的索引项在文档中未出现，1表示对应的索引项在文档中出现。</li>
<li>查询表示：和文档表示相同，每个元素表示相应索引项是否在查询中出现。</li>
<li>项-文档矩阵：行数是索引项个数，列数是文档个数，元素 <script type="math/tex">f_{i,j}</script> 表示第 <script type="math/tex">i</script> 个索引项在第 <script type="math/tex">j</script> 个文档中出现的频数。显然，在词袋方法中是0-1矩阵。</li>
<li>文档的逻辑视图：大概就是表示文档的方式。通常是把文档全文转变成索引项集合，流程如下图：<br><a href="https://file.shivakasu.cn/4433d3fae9363c920f64/mir2-1.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="mir2-1" class="fancybox"><img alt="mir2-1" data-src="https://file.shivakasu.cn/4433d3fae9363c920f64/mir2-1.png" class="lazyload" title="mir2-1"></a></li>
</ul>
<h3 id="2-2-布尔模型"><a href="#2-2-布尔模型" class="headerlink" title="2.2 布尔模型"></a>2.2 布尔模型</h3><p>&emsp;&emsp;使用词袋方法表示文档，用析取范式(disjunct normal form)表示查询。例如对于词汇表 <script type="math/tex">V=(k_a,k_b,k_c,k_d)</script> 和查询 <script type="math/tex">q=k_a\wedge (k_b\vee k_c)</script> ，用析取范式表示查询为：</p>
<script type="math/tex; mode=display">q_{DNF}=(1,1,1,0)\vee(1,1,1,1)\vee(1,1,0,0)\vee(1,1,0,1)\vee(1,0,0,0)\vee(1,0,0,1)</script><p>文档与查询的相关度定义为析取范式中是否有一项与文档表示相同，因此也是二值的。</p>
<p>&emsp;&emsp;布尔模型的优点是简单，缺点是不支持排序。因为相关度是二值的，只能表示相关与否，而不能表示相关的程度。</p>
<h3 id="2-3-项权重"><a href="#2-3-项权重" class="headerlink" title="2.3 项权重"></a>2.3 项权重</h3><p>&emsp;&emsp;提高检索质量的一个方法是给每个索引项设置权重，通常根据索引项在整个文档集中出现的频次设置权重。如果不假设索引项之间相互独立，还要考虑索引项之间的相关性，因为索引项之间的关联往往会反映文档之间的关联，一种计算项间相关性的方法是项-文档矩阵乘他的转置矩阵，如下图：<br><a href="https://file.shivakasu.cn/0149fd92db89c2e04630/mir2-2.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="mir2-2" class="fancybox"><img alt="mir2-2" data-src="https://file.shivakasu.cn/0149fd92db89c2e04630/mir2-2.png" class="lazyload" title="mir2-2"></a><br>假设项间相互独立可以简化模型、提高计算效率，而利用项间相关性提高排序水平也是十分复杂的工作，考虑了项间相关性并不能保证排序水平的提高，因此是否假设项间相互独立没有固定的标准。</p>
<h3 id="2-4-TD-IDF"><a href="#2-4-TD-IDF" class="headerlink" title="2.4 TD-IDF"></a>2.4 TD-IDF</h3><p>&emsp;&emsp;TF-IDF是一个常用的计算项权重的指标，其中TF(Term frequency)表示项频，IDF(Inverse document frequency)表示反比文档频率。</p>
<p>&emsp;&emsp;使用项频是基于Luhn假设，即高频项对描述文档的关键主题是重要的。可以直接将索引项的频次作为TF权重，即 <script type="math/tex">tf_{i,j}=f_{i,j}</script> ，但考虑到要与IDF权重结合，而IDF使用了对数运算，因此通常使用TF权重的一个变种：</p>
<script type="math/tex; mode=display">tf_{i,j}=\begin{cases}
1+log_2f_{i,j} & f_{i,j}>0 \\
0 & otherwise
\end{cases}</script><p>&emsp;&emsp;TF权重倾向于给频次高的索引项更大的权重，但也要考虑索引项的区分度，即索引项特异性(term specificity)。如果一个索引项在每个文档中都出现，虽然出现频次高，但是对于文档排序等任务没有太大帮助，最常见的就是a、the这样的冠词、连词和介词。因此不仅要考虑高频项，还要考虑区分度大的索引项。IDF权重考虑的就是某个索引项在多少个文档中出现，即相对文档频率 <script type="math/tex">n_i/N</script> ， <script type="math/tex">IDF_i=log_2\frac{N}{n_i}</script> ，其中 <script type="math/tex">N</script> 是文档集中的文档数量， <script type="math/tex">n_i</script> 是出现索引项 <script type="math/tex">k_i</script> 的文档数量，因为相对文档频率越小的索引项区分度越大，所以IDF使用了相对文档频率的倒数，称作反比文档频率。</p>
<p>&emsp;&emsp;TF-IDF将二者结合起来，计算方法如下：</p>
<script type="math/tex; mode=display">w_{i,j}=\begin{cases}
(1+log_2f_{i,j})*log_2\frac{N}{n_i} & f_{i,j}>0 \\
0 & otherwise
\end{cases}</script><p>&emsp;&emsp;TF、IDF和TF-IDF有多种变体。TF变体如下：<br><a href="https://file.shivakasu.cn/6a3557149d63d9d07c8d/mir2-3.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="mir2-3" class="fancybox"><img alt="mir2-3" data-src="https://file.shivakasu.cn/6a3557149d63d9d07c8d/mir2-3.png" class="lazyload" title="mir2-3"></a></p>
<p>IDF变体如下：<br><a href="https://file.shivakasu.cn/9b5b06f2b588a7e7566c/mir2-4.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="mir2-4" class="fancybox"><img alt="mir2-4" data-src="https://file.shivakasu.cn/9b5b06f2b588a7e7566c/mir2-4.png" class="lazyload" title="mir2-4"></a></p>
<p>TF-IDF变体如下：<br><a href="https://file.shivakasu.cn/4a396f3ce65a26f1b49a/mir2-5.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="mir2-5" class="fancybox"><img alt="mir2-5" data-src="https://file.shivakasu.cn/4a396f3ce65a26f1b49a/mir2-5.png" class="lazyload" title="mir2-5"></a></p>
<p>&emsp;&emsp;通过下图可以分析出TF-IDF的性质。TF和IDF权重表现出的幂律特性会相互平衡，高TF权重趋于和低IDF权重结合，低TF权重趋于和高IDF权重结合，结果是TF-IDF权重最高的索引项往往具有中等TF和IDF权重，而项频太高的项和文档频率太低的项经过平衡后都具有较低的TF-IDF权重。妙啊！<br><a href="https://file.shivakasu.cn/ded9527fd90145d96807/mir2-6.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="mir2-6" class="fancybox"><img alt="mir2-6" data-src="https://file.shivakasu.cn/ded9527fd90145d96807/mir2-6.png" class="lazyload" title="mir2-6"></a></p>
<h3 id="2-5-文档长度归一化"><a href="#2-5-文档长度归一化" class="headerlink" title="2.5 文档长度归一化"></a>2.5 文档长度归一化</h3><p>&emsp;&emsp;对于给定的查询，较长的文档仅仅因为包含更多的索引项而更可能被检出，为了消除这一影响，可以把文档的排序除以其长度，这个过程称为文档长度归一化，如何计算文档长度取决于文档的表示形式。</p>
<h3 id="2-6-向量模型"><a href="#2-6-向量模型" class="headerlink" title="2.6 向量模型"></a>2.6 向量模型</h3><p>&emsp;&emsp;布尔模型使用析取范式的每一项和文档表示进行严格匹配，难以得到理想的结果。向量模型将文档和查询表示为向量形式，使用向量夹角的余弦值衡量相似度，成功将相似度量化为可用于比较和排序的数值，基于相似度的排序可以理解为一种部分匹配策略。文档的向量表示为 <script type="math/tex">\vec{d_j}=(w_{1,j},w_{2,j},\cdots,w_{t,j})</script> ，其中 <script type="math/tex">t</script> 是索引项总个数， <script type="math/tex">w_{i,j}</script>是项-文档对 <script type="math/tex">(k_i,d_j)</script> 的权重，一般采用TF-IDF权重，查询的向量表示为 <script type="math/tex">\vec{q}=(w_{1,q},w_{2,q},\cdots,w_{t,q})</script> , <script type="math/tex">w_{i,q}</script>是项-查询对 <script type="math/tex">(k_i,q)</script> 的权重。文档-查询余弦相似度公式为：</p>
<script type="math/tex; mode=display">sim(d_j,q)=\frac{\vec{d_j}\cdot\vec{q}}{|\vec{d_j}|\times|\vec{q}|}</script><p>注意到余弦公式分母的向量范数恰好也起到了文档长度归一化的作用。</p>
<h3 id="2-7-概率模型"><a href="#2-7-概率模型" class="headerlink" title="2.7 概率模型"></a>2.7 概率模型</h3><p>&emsp;&emsp;概率模型的目标是估计文档与查询相关的概率，他假定这种相关性仅依赖于文档和查询本身的表示，并假定存在一个理想答案集，仅包含所有与查询相关的文档，因此能够最大化与用户相关的总体概率。显然，这种假设是对真实情况的简化，所以必然会存在一些缺陷。</p>
<p>&emsp;&emsp;概率模型计算相关度的公式是：</p>
<script type="math/tex; mode=display">sim(d_j,q)=\frac{P(R|\vec{d_j})}{P(\overline{R}|\vec{d_j})}</script><p>其中 <script type="math/tex">R</script> 是与查询 <script type="math/tex">q</script> 相关的文档的集合， <script type="math/tex">\overline{R}</script> 是与查询 <script type="math/tex">q</script> 不相关的文档的集合， <script type="math/tex">P(R|\vec{d_j})</script>是文档 <script type="math/tex">d_j</script> 与查询 <script type="math/tex">q</script> 相关的概率， <script type="math/tex">P(\overline{R}|\vec{d_j})</script>是文档 <script type="math/tex">d_j</script> 与查询 <script type="math/tex">q</script> 不相关的概率。根据贝叶斯公式：</p>
<script type="math/tex; mode=display">sim(d_j,q)=\frac{P(\vec{d_j}|R,q)\times P(R,q)}{P(\vec{d_j}|\overline{R},q)\times P(\overline{R},q)}\sim\frac{P(\vec{d_j}|R,q)}{P(\vec{d_j}|\overline{R},q)}</script><p>其中 <script type="math/tex">P(\vec{d_j}|R,q)</script> 表示从查询 <script type="math/tex">q</script> 的相关文档集 <script type="math/tex">R</script> 中随机选择的一偏文档表示为 <script type="math/tex">\vec{d_j}</script> 的概率， <script type="math/tex">P(R,q)</script> 表示从整个文档集中随机选择的文档和查询 <script type="math/tex">q</script> 相关的概率， <script type="math/tex">P(\vec{d_j}|\overline{R},q)</script> 和 <script type="math/tex">P(\overline{R},q)</script> 的含义是相似且互补的。概率模型中不考虑项权重，所以 <script type="math/tex">\vec{d_j}</script> 是一个二值向量，如果假设索引项间的独立性，即所谓的二值独立假设，可以得到：</p>
<script type="math/tex; mode=display">sim(d_j,q)\sim\frac{(\prod_{k_i|w_{i,j}=1}p_{iR})\times(\prod_{k_i|w_{i,j}=0}(1-p_{iR}))}{(\prod_{k_i|w_{i,j}=1}q_{iR})\times(\prod_{k_i|w_{i,j}=0}(1-q_{iR}))}</script><p>其中 <script type="math/tex">p_{iR}</script> 表示索引项 <script type="math/tex">k_i</script> 出现在从查询 <script type="math/tex">q</script> 的相关文档集 <script type="math/tex">R</script> 中随机选择的一偏文档内的概率， <script type="math/tex">q_{iR}</script> 表示索引项 <script type="math/tex">k_i</script> 出现在从查询 <script type="math/tex">q</script> 的不相关文档集 <script type="math/tex">\overline{R}</script> 中随机选择的一偏文档内的概率。使用对数函数只改变数值而不改变排序结果，所以可以进一步简化为：</p>
<script type="math/tex; mode=display">sim(d_j,q)\sim\sum_{k_i\in q\wedge k_i\in d_j}log(\frac{p_{iR}}{1-p_{iR}})+log(\frac{1-q_{iR}}{q_{iR}})</script><p>得到了相似度公式，接下来就是如何计算 <script type="math/tex">p_{iR}</script> 和 <script type="math/tex">q_{iR}</script> 。</p>
<p>&emsp;&emsp;一种计算方法是使用索引项出现列联表，如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">情况</th>
<th style="text-align:center">相关文档数</th>
<th style="text-align:center">不相关文档数</th>
<th style="text-align:center">总文档数</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">包含 $k_i$ 的文档</td>
<td style="text-align:center">$r_i$</td>
<td style="text-align:center">$n_i-r_i$</td>
<td style="text-align:center">$n_i$</td>
</tr>
<tr>
<td style="text-align:center">不包含 $k_i$ 的文档</td>
<td style="text-align:center">$R-r_i$</td>
<td style="text-align:center">$N-n_i-(R-r_i)$</td>
<td style="text-align:center">$N-n_i$</td>
</tr>
<tr>
<td style="text-align:center">所有文档</td>
<td style="text-align:center">$R$</td>
<td style="text-align:center">$N-R$</td>
<td style="text-align:center">$N$</td>
</tr>
</tbody>
</table>
</div>
<p>那么可以得到，</p>
<script type="math/tex; mode=display">p_{iR}=\frac{r_i}{R}\quad, \quad q_{iR}=\frac{n_i-r_i}{N-R}</script><script type="math/tex; mode=display">sim(d_j,q)\sim\sum_{k_i\in q\wedge k_i\in d_j}log(\frac{(r_i+0.5)(N-n_i-R+r_i+0.5)}{(R-r_i+0.5)(n_i-r_i+0.5)})</script><p>之所以给每个包含 <script type="math/tex">r_i</script> 的项加0.5，是为了减小极端情况下过小的 <script type="math/tex">r_i</script> 对 <script type="math/tex">log</script> 计算的影响。这种方法需要人工估计 <script type="math/tex">r_i</script> 和 <script type="math/tex">R</script> 值，所以不实用，同时缺少文档长度归一化的操作，使得排序效果也不是很好。</p>
<p>&emsp;&emsp;另一种方法是在避免人工估计的条件下，基于几条假设来自动更新 <script type="math/tex">r_i</script> 和 <script type="math/tex">R</script> 值，个人认为这里的假设太牵强，理解不了。</p>
<p>&emsp;&emsp;概率模型的优点是能按照相关概率进行排序，但其认为相关性仅与文档和查询的内容有关，所以实际应用时效果难以保证。此外，概率模型不可避免地要做初始估计将文档分为相关和不相关集合，不太好操作。观察上面计算 <script type="math/tex">sim(d_j,q)</script> 的公式，与IDF权重的公式是相似的，从这个角度看，概率模型的另一个缺点是没有用到TF特征，也没有进行文档长度归一化。</p>
<h2 id="3-其他集合论模型"><a href="#3-其他集合论模型" class="headerlink" title="3 其他集合论模型"></a>3 其他集合论模型</h2><h3 id="3-1-基于集合的模型"><a href="#3-1-基于集合的模型" class="headerlink" title="3.1 基于集合的模型"></a>3.1 基于集合的模型</h3><p>&emsp;&emsp;基于集合的模型不考虑单独的索引项，而是考虑索引项之间的相互依赖性，通过引入项集的概念表示索引项之间的关联。</p>
<p>&emsp;&emsp;项集(Termset)：项集 <script type="math/tex">S_i=\{k_a,k_b,\cdots,k_n\}</script> 是文档集中索引项的子集。若 <script type="math/tex">S_i</script> 中所有的索引项都出现在文档 <script type="math/tex">d_j</script> 中，就称项集 <script type="math/tex">S_i</script> 出现在 <script type="math/tex">d_j</script> 中。</p>
<p>&emsp;&emsp;显然，若文档集中有 <script type="math/tex">t</script> 个索引项，则理论上有 <script type="math/tex">2^t</script> 个项集，但实际数据集中一般仅包含部分项集。同时，用项集表示替代索引项表示就需要把项的词汇表改为项集的词汇表，即 <script type="math/tex">V_S=\{S_1,S_2,\cdots,S_{2^t}\}</script> 。</p>
<p>&emsp;&emsp;频繁项集(Frequent termsets)：由 <script type="math/tex">n</script> 个项构成的项集称为 <script type="math/tex">n</script> 项集，如果包含某个 <script type="math/tex">n</script> 项集的文档数 <script type="math/tex">\mathcal{N}_i</script> 高于某个给定的阈值，那么这个 <script type="math/tex">n</script> 项集 <script type="math/tex">S_i</script> 称为是频繁的。显然，一个 <script type="math/tex">n</script> 项集是频繁的当且仅当他的所有 <script type="math/tex">(n-1)</script> 项集都是频繁地。</p>
<p>&emsp;&emsp;在TF-IDF中，计算的权重是项-文档矩阵的元素，在集合模型中与之类似，计算的权重是项集-文档矩阵的元素。对于 <script type="math/tex">(S_i,d_j)</script> ，令 <script type="math/tex">N</script> 是文档集中文档总数，<script type="math/tex">\mathcal{F}_{i,j}</script> 是项集 <script type="math/tex">S_i</script> 在文档 <script type="math/tex">d_j</script> 中的原始出现频率，赋予项集权重：</p>
<script type="math/tex; mode=display">\mathcal{W}_{i,j}=\begin{cases}
(1+log_2\mathcal{F}_{i,j})*log_2(1+\frac{N}{\mathcal{N}_i}) & \mathcal{F}_{i,j}>0 \\
0 & otherwise
\end{cases}</script><p>同理， <script type="math/tex">\vec{d_j}=(\mathcal{W}_{1,j},\mathcal{W}_{2,j},\cdots,\mathcal{W}_{2^t,j})</script> ， <script type="math/tex">\vec{q}=(\mathcal{W}_{1,q},\mathcal{W}_{2,q},\cdots,\mathcal{W}_{2^t,q})</script>，相似度计算公式为：</p>
<script type="math/tex; mode=display">sim(d_j,q)=\frac{\vec{d_j}\cdot\vec{q}}{|\vec{d_j}|\times|\vec{q}|}=\frac{\sum_{S_i}\mathcal{W}_{i,j}\times\mathcal{W}_{i,q}}{|\vec{d_j}|\times|\vec{q}|}</script><p>由于项集空间是项空间的指数级大小，所以相似度的计算十分复杂，需要进行计算简化。例如在计算向量范数时只考虑 <script type="math/tex">1</script> 项集。或是进一步缩小项集的范围，只考虑频繁闭项集，闭项集(Closed termset)就是项集的闭包，比如项集 <script type="math/tex">\{k_1\}</script> 、 <script type="math/tex">\{k_2\}</script> 、 <script type="math/tex">\{k_1,k_2\}</script>出现在相同的文档子集中，那么可以只计算 <script type="math/tex">\{k_1,k_2\}</script>，大大减小了计算量，除了频繁闭项集，还可选择最大频繁集，即添加任何索引项都不能使其保持频繁性。从项集数目上看，频繁项集&gt;频繁闭项集&gt;最大频繁集，需要注意的是，减少计算必然伴随着信息的损失，因此需要根据实际情况进行权衡。</p>
<h3 id="3-2-扩展布尔模型"><a href="#3-2-扩展布尔模型" class="headerlink" title="3.2 扩展布尔模型"></a>3.2 扩展布尔模型</h3><p>&emsp;&emsp;用向量模型的特征扩展布尔模型，狗尾续貂？</p>
<h3 id="3-3-模糊集模型"><a href="#3-3-模糊集模型" class="headerlink" title="3.3 模糊集模型"></a>3.3 模糊集模型</h3><p>&emsp;&emsp;模糊集模型基于模糊集理论，对于每一个索引项 <script type="math/tex">k_i</script> ，为其分配一个模糊集(fuzzy set) <script type="math/tex">D_i</script> ，模糊集为每一个文档 <script type="math/tex">d_j</script> 分配一个介于区间 <script type="math/tex">[0,1]</script> 之间的隶属度(degree of membership)  <script type="math/tex">\mu_{i,j}</script> ，若 <script type="math/tex">\mu_{i,j}\sim 1</script> 表明 <script type="math/tex">k_i</script> 是 <script type="math/tex">d_j</script> 的良好模糊索引项，若 <script type="math/tex">\mu_{i,j}\sim 0</script> 表明 <script type="math/tex">k_i</script> 不是 <script type="math/tex">d_j</script> 的良好模糊索引项。隶属度可以通过项间相关性矩阵 <script type="math/tex">C</script> 来计算，索引项 <script type="math/tex">k_i</script> 和 <script type="math/tex">k_l</script> 的相关性计算公式为：</p>
<script type="math/tex; mode=display">c_{i,l}=\frac{n_{i,l}}{n_i+n_l-n_{i,l}}</script><p>其中 <script type="math/tex">n_i</script> 是含有索引项 <script type="math/tex">k_i</script> 的文档数， <script type="math/tex">n_l</script> 是含有索引项 <script type="math/tex">k_l</script> 的文档数， <script type="math/tex">n_{i,l}</script> 是同时含有这两个索引项的文档数，这种相关性度量被广泛应用在聚类算法中。有了相关性度量，就可以计算隶属度：</p>
<script type="math/tex; mode=display">\mu_{i,j}=1-\prod_{k_l\in d_j}(1-c_{i,l})</script><p>这其实就是在考虑 <script type="math/tex">k_i</script> 和 <script type="math/tex">d_j</script> 中每一个索引项的相关性，可以看出，只要 <script type="math/tex">d_j</script> 中至少有一个索引项 <script type="math/tex">k_l</script> 与 <script type="math/tex">k_i</script> 关系密切(即 <script type="math/tex">c_{i,l}\sim 1</script> )，则 <script type="math/tex">\mu_{i,j}\sim 1</script> 。此外，采用代数和的方式计算而不是对所有相关性使用 <script type="math/tex">max</script> 函数，可以使 <script type="math/tex">\mu_{i,j}</script> 的值变得平滑。</p>
<p>&emsp;&emsp;有了文档相对索引项的隶属度，就可以进一步计算文档相对于查询的隶属度，因为借鉴布尔模型的方法，查询可以表示成索引项组成的逻辑表达式。例如对于查询 <script type="math/tex">q=k_a\wedge (k_b\vee\neg k_c)</script> ，可以写成析取范式 <script type="math/tex">\vec{q}_{dnf}=(1,1,1)\vee(1,1,0)\vee(1,0,0)</script> ，设 <script type="math/tex">D_a</script> 、 <script type="math/tex">D_b</script> 、 <script type="math/tex">D_c</script> 分别是 <script type="math/tex">k_a</script> 、 <script type="math/tex">k_b</script> 、 <script type="math/tex">k_c</script> 的模糊集，查询的模糊集 <script type="math/tex">D_q</script> 可以从下图理解：<br><a href="https://file.shivakasu.cn/3d166e41cb81bea90ac8/mir2-7.png" target="_blank" rel="noopener" data-fancybox="group" data-caption="mir2-7" class="fancybox"><img alt="mir2-7" data-src="https://file.shivakasu.cn/3d166e41cb81bea90ac8/mir2-7.png" class="lazyload" title="mir2-7"></a><br>其中， <script type="math/tex">cc_1=\mu_{a,j}\mu_{b,j}\mu_{c,j}</script> 、 <script type="math/tex">cc_2=\mu_{a,j}\mu_{b,j}(1-\mu_{c,j})</script> 、 <script type="math/tex">cc_3=\mu_{a,j}(1-\mu_{b,j})(1-\mu_{c,j})</script> ，则：</p>
<script type="math/tex; mode=display">\mu_{q,j}=1-\prod_{i=1}^3(1-cc_i)</script><p>同样，采用代数和的方式计算而不是对所有相关性使用 <script type="math/tex">max</script> 函数，可以使<script type="math/tex">\mu_{q,j}</script> 的值变得平滑。</p>
<h2 id="4-其他代数模型"><a href="#4-其他代数模型" class="headerlink" title="4 其他代数模型"></a>4 其他代数模型</h2><h3 id="4-1-广义向量空间模型"><a href="#4-1-广义向量空间模型" class="headerlink" title="4.1 广义向量空间模型"></a>4.1 广义向量空间模型</h3></div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:wkx1996@foxmail.com">w.k.x.</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://shivakasu.github.io/2019/01/24/mir2/">http://shivakasu.github.io/2019/01/24/mir2/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://shivakasu.github.io">SHIVAKASU</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%90%86%E8%AE%BA/">计算机理论    </a></div><div class="post_share"><div class="social-share" data-image="https://file.shivakasu.cn/431c4e80e82294b29130/mir.png" data-sites="facebook,twitter,wechat,weibo,qq,qzone,douban,google,linkedin"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-button button--primary button--animated"> <i class="fa fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lazyload post-qr-code__img" src="https://file.shivakasu.cn/4a84dcfb31806db98b2a/wechat.png" alt="微信"><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lazyload post-qr-code__img" src="https://file.shivakasu.cn/982f781ef31cbc46968b/alipay.jpg" alt="支付宝"><div class="post-qr-code__desc">支付宝</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="prev-post pull_left"><a href="/2019/02/01/mir3/"><img class="prev_cover lazyload" data-src="https://file.shivakasu.cn/431c4e80e82294b29130/mir.png" onerror="onerror=null;src='/img/404.jpg'"><div class="label">上一篇</div><div class="prev_info"><span>现代信息检索 Chapter 4：检索评价</span></div></a></div><div class="next-post pull_right"><a href="/2019/01/19/mir1/"><img class="next_cover lazyload" data-src="https://file.shivakasu.cn/431c4e80e82294b29130/mir.png" onerror="onerror=null;src='/img/404.jpg'"><div class="label">下一篇</div><div class="next_info"><span>现代信息检索 Chapter 2：用户搜索界面</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2019/01/18/mir0/" title="现代信息检索 Chapter 1：引言"><img class="relatedPosts_cover lazyload"data-src="https://file.shivakasu.cn/431c4e80e82294b29130/mir.png"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2019-01-18</div><div class="relatedPosts_title">现代信息检索 Chapter 1：引言</div></div></a></div><div class="relatedPosts_item"><a href="/2019/01/19/mir1/" title="现代信息检索 Chapter 2：用户搜索界面"><img class="relatedPosts_cover lazyload"data-src="https://file.shivakasu.cn/431c4e80e82294b29130/mir.png"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2019-01-19</div><div class="relatedPosts_title">现代信息检索 Chapter 2：用户搜索界面</div></div></a></div><div class="relatedPosts_item"><a href="/2019/02/01/mir3/" title="现代信息检索 Chapter 4：检索评价"><img class="relatedPosts_cover lazyload"data-src="https://file.shivakasu.cn/431c4e80e82294b29130/mir.png"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2019-02-01</div><div class="relatedPosts_title">现代信息检索 Chapter 4：检索评价</div></div></a></div><div class="relatedPosts_item"><a href="/2019/02/03/mir4/" title="现代信息检索 Chapter 6：文档-语言及属性"><img class="relatedPosts_cover lazyload"data-src="https://file.shivakasu.cn/431c4e80e82294b29130/mir.png"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2019-02-03</div><div class="relatedPosts_title">现代信息检索 Chapter 6：文档-语言及属性</div></div></a></div></div><div class="clear_both"></div></div><hr><div id="post-comment"><div class="comment_headling"><i class="fa fa-comments fa-fw" aria-hidden="true"></i><span> 评论</span></div><div class="vcomment" id="vcomment"></div><script src="https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js"></script><script>var notify = false == true ? true : false;
var verify = true == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;

window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'qIfwEiuSjlc5vyminB1rx2qX-gzGzoHsz',
  appKey:'erfVXuW3AybzbWGqxSGxnBRp',
  placeholder:'来都来了，说点儿什么吧~',
  avatar:'monsterid',
  guest_info:guest_info,
  pageSize:'10',
  lang:'zh-cn',
  recordIP: true
});</script></div></div></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2020 By w.k.x.</div><div class="framework-info"><span>driven by </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a></div><div class="icp"><a href="http://www.beian.miit.gov.cn/state/outPortal/loginPortal.action" target="_blank" rel="noopener"><img class="icp-icon" src="/img/icp.png"><span>京ICP备19001969号-1</span></a></div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换" target="_self">繁</a><i class="darkmode fa fa-moon-o" id="darkmode" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><a id="to_comment" href="#post-comment" title="直达评论"><i class="scroll_to_comment fa fa-comments">  </i></a><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/js/fireworks.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/koharu.model.json"},"display":{"position":"left","width":150,"height":300},"mobile":{"show":false},"react":{"opacity":0.7},"log":false});</script></body></html>